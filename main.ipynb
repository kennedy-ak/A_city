{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AFRIMASH CUSTOMER INTELLIGENCE CHALLENGE\n",
    "## Complete End-to-End Analysis Pipeline\n",
    "\n",
    "**Objective:** Build a comprehensive customer intelligence system that:\n",
    "1. Segments customers into meaningful groups\n",
    "2. Predicts customer behavior and purchasing patterns\n",
    "3. Analyzes and improves customer retention rates\n",
    "4. Generates personalized product recommendations\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 PART 1: Setup & Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine Learning libraries\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
    "    mean_squared_error, r2_score, mean_absolute_error, accuracy_score, silhouette_score\n",
    ")\n",
    "\n",
    "# For recommendations\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import pickle\n",
    "\n",
    "# Visualization settings\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (16, 10)\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📁 PART 2: Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "print(\"Loading datasets...\")\n",
    "\n",
    "try:\n",
    "    rfm_df = pd.read_excel('Copy of RFM_Data.xlsx')\n",
    "    trans_df = pd.read_excel('Copy of Transaction_Data.xlsx')\n",
    "    \n",
    "    print(f\"✅ RFM Data loaded: {rfm_df.shape[0]:,} customers, {rfm_df.shape[1]} columns\")\n",
    "    print(f\"✅ Transaction Data loaded: {trans_df.shape[0]:,} transactions, {trans_df.shape[1]} columns\")\nexcept FileNotFoundError as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    print(\"Please ensure the Excel files are in the same directory as this notebook.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 PART 3: Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM Data Overview\n",
    "print(\"=\"*80)\n",
    "print(\"RFM DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(rfm_df.head())\n",
    "\n",
    "print(\"\\nData Info:\")\n",
    "print(rfm_df.info())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(rfm_df.describe())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_rfm = rfm_df.isnull().sum()\n",
    "print(missing_rfm[missing_rfm > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction Data Overview\n",
    "print(\"=\"*80)\n",
    "print(\"TRANSACTION DATA OVERVIEW\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "display(trans_df.head())\n",
    "\n",
    "print(\"\\nData Info:\")\n",
    "print(trans_df.info())\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(trans_df.describe())\n",
    "\n",
    "print(\"\\nMissing Values:\")\n",
    "missing_trans = trans_df.isnull().sum()\n",
    "print(missing_trans[missing_trans > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 PART 4: Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"DATA CLEANING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 4.1 Handle missing products\n",
    "if 'Product(s)' in trans_df.columns:\n",
    "    missing_products = trans_df['Product(s)'].isnull().sum()\n",
    "    trans_df['Product(s)'].fillna(\"Unknown Product\", inplace=True)\n",
    "    print(f\"✓ Filled {missing_products:,} missing product names with 'Unknown Product'\")\n",
    "\n",
    "# 4.2 Check for negative values\n",
    "if 'Revenue' in trans_df.columns:\n",
    "    negative_revenue = trans_df[trans_df['Revenue'] < 0]\n",
    "    print(f\"✓ Found {len(negative_revenue):,} negative revenue transactions (refunds/returns)\")\n",
    "\n",
    "if 'Monetary' in rfm_df.columns:\n",
    "    negative_monetary = rfm_df[rfm_df['Monetary'] < 0]\n",
    "    print(f\"✓ Found {len(negative_monetary):,} customers with negative monetary values\")\n",
    "\n",
    "# 4.3 Remove duplicates\n",
    "if 'Order #' in trans_df.columns:\n",
    "    initial_trans = len(trans_df)\n",
    "    trans_df = trans_df.drop_duplicates(subset=['Order #'], keep='first')\n",
    "    print(f\"✓ Removed {initial_trans - len(trans_df):,} duplicate orders\")\n",
    "\n",
    "# 4.4 Convert date columns\n",
    "if 'Date' in trans_df.columns:\n",
    "    trans_df['Date'] = pd.to_datetime(trans_df['Date'])\n",
    "    print(f\"✓ Converted Date column to datetime\")\n",
    "\n",
    "print(\"\\n✅ Data cleaning complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ PART 5: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FEATURE ENGINEERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 5.1 Calculate Recency\n",
    "current_date = trans_df['Date'].max()\n",
    "print(f\"\\nAnalysis Date: {current_date}\")\n",
    "\n",
    "last_purchase = trans_df.groupby('Customer_ID')['Date'].max().reset_index()\n",
    "last_purchase.columns = ['Customer_ID', 'Last_Purchase_Date']\n",
    "last_purchase['Recency'] = (current_date - last_purchase['Last_Purchase_Date']).dt.days\n",
    "\n",
    "rfm_df = rfm_df.merge(last_purchase[['Customer_ID', 'Recency', 'Last_Purchase_Date']], \n",
    "                       on='Customer_ID', how='left')\n",
    "print(f\"✓ Added Recency (days since last purchase)\")\n",
    "\n",
    "# 5.2 Extract time features\n",
    "trans_df['Year'] = trans_df['Date'].dt.year\n",
    "trans_df['Month'] = trans_df['Date'].dt.month\n",
    "trans_df['Quarter'] = trans_df['Date'].dt.quarter\n",
    "trans_df['DayOfWeek'] = trans_df['Date'].dt.dayofweek\n",
    "trans_df['DayName'] = trans_df['Date'].dt.day_name()\n",
    "trans_df['MonthName'] = trans_df['Date'].dt.month_name()\n",
    "print(\"✓ Added time-based features\")\n",
    "\n",
    "# 5.3 First purchase date\n",
    "first_purchase = trans_df.groupby('Customer_ID')['Date'].min().reset_index()\n",
    "first_purchase.columns = ['Customer_ID', 'First_Purchase_Date']\n",
    "rfm_df = rfm_df.merge(first_purchase, on='Customer_ID', how='left')\n",
    "print(\"✓ Added First Purchase Date\")\n",
    "\n",
    "# 5.4 Customer Age\n",
    "rfm_df['Customer_Age_Days'] = (current_date - rfm_df['First_Purchase_Date']).dt.days\n",
    "print(\"✓ Added Customer Age\")\n",
    "\n",
    "# 5.5 Frequency Categories\n",
    "def categorize_frequency(freq):\n",
    "    if freq == 1:\n",
    "        return 'One-time'\n",
    "    elif freq <= 3:\n",
    "        return 'Low'\n",
    "    elif freq <= 10:\n",
    "        return 'Medium'\n",
    "    elif freq <= 20:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Very High'\n",
    "\n",
    "rfm_df['Frequency_Category'] = rfm_df['Frequency'].apply(categorize_frequency)\n",
    "print(\"✓ Added Frequency Category\")\n",
    "\n",
    "# 5.6 Monetary Categories\n",
    "def categorize_monetary(value):\n",
    "    if value < 50000:\n",
    "        return 'Low Value'\n",
    "    elif value < 200000:\n",
    "        return 'Medium Value'\n",
    "    elif value < 1000000:\n",
    "        return 'High Value'\n",
    "    else:\n",
    "        return 'Very High Value'\n",
    "\n",
    "rfm_df['Monetary_Category'] = rfm_df['Monetary'].apply(categorize_monetary)\n",
    "print(\"✓ Added Monetary Category\")\n",
    "\n",
    "# 5.7 Recency Categories\n",
    "def categorize_recency(recency):\n",
    "    if recency <= 30:\n",
    "        return 'Active (0-30 days)'\n",
    "    elif recency <= 90:\n",
    "        return 'Recent (31-90 days)'\n",
    "    elif recency <= 180:\n",
    "        return 'Cooling (91-180 days)'\n",
    "    elif recency <= 365:\n",
    "        return 'At Risk (181-365 days)'\n",
    "    else:\n",
    "        return 'Lost (>365 days)'\n",
    "\n",
    "rfm_df['Recency_Category'] = rfm_df['Recency'].apply(categorize_recency)\n",
    "print(\"✓ Added Recency Category\")\n",
    "\n",
    "# 5.8 Product Categories\n",
    "def extract_product_category(product_text):\n",
    "    if pd.isna(product_text) or product_text == \"Unknown Product\":\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    product_lower = str(product_text).lower()\n",
    "    \n",
    "    if any(word in product_lower for word in ['chick', 'pullet', 'broiler', 'poultry', 'bird']):\n",
    "        return 'Poultry'\n",
    "    elif any(word in product_lower for word in ['seed', 'maize', 'corn', 'soybean', 'rice']):\n",
    "        return 'Seeds'\n",
    "    elif any(word in product_lower for word in ['feed', 'concentrate', 'premix']):\n",
    "        return 'Feed'\n",
    "    elif any(word in product_lower for word in ['fertilizer', 'npk', 'urea']):\n",
    "        return 'Fertilizer'\n",
    "    elif any(word in product_lower for word in ['pesticide', 'herbicide', 'insecticide', 'fungicide']):\n",
    "        return 'Agrochemicals'\n",
    "    elif any(word in product_lower for word in ['tractor', 'machine', 'equipment', 'processing', 'pump']):\n",
    "        return 'Equipment'\n",
    "    elif any(word in product_lower for word in ['vegetable', 'tomato', 'pepper', 'cucumber', 'carrot']):\n",
    "        return 'Vegetables'\n",
    "    elif any(word in product_lower for word in ['fruit', 'papaya', 'mango', 'watermelon']):\n",
    "        return 'Fruits'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "trans_df['Product_Category'] = trans_df['Product(s)'].apply(extract_product_category)\n",
    "print(\"✓ Extracted Product Categories\")\n",
    "\n",
    "# 5.9 Category counts per customer\n",
    "category_counts = trans_df.groupby('Customer_ID')['Product_Category'].value_counts().unstack(fill_value=0)\n",
    "category_counts.columns = [f'Category_{col}' for col in category_counts.columns]\n",
    "rfm_df = rfm_df.merge(category_counts, on='Customer_ID', how='left')\n",
    "rfm_df = rfm_df.fillna(0)\n",
    "print(\"✓ Added product category purchase counts\")\n",
    "\n",
    "# 5.10 Days between purchases\n",
    "rfm_df['Days_Between_Purchases'] = rfm_df['Customer_Age_Days'] / rfm_df['Frequency'].replace(0, 1)\n",
    "print(\"✓ Added average days between purchases\")\n",
    "\n",
    "print(\"\\n✅ Feature engineering complete!\")\n",
    "print(f\"\\nTotal features in RFM dataset: {rfm_df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 PART 6: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Customer Overview Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CUSTOMER OVERVIEW STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n--- Customer Type Distribution ---\")\n",
    "print(rfm_df['Customer_Type'].value_counts())\n",
    "print(f\"\\nNew Customers: {(rfm_df['Customer_Type']=='new').sum()/len(rfm_df)*100:.1f}%\")\n",
    "print(f\"Returning Customers: {(rfm_df['Customer_Type']=='returning').sum()/len(rfm_df)*100:.1f}%\")\n",
    "\n",
    "print(\"\\n--- Frequency Category Distribution ---\")\n",
    "print(rfm_df['Frequency_Category'].value_counts())\n",
    "\n",
    "print(\"\\n--- Monetary Category Distribution ---\")\n",
    "print(rfm_df['Monetary_Category'].value_counts())\n",
    "\n",
    "print(\"\\n--- Recency Category Distribution ---\")\n",
    "print(rfm_df['Recency_Category'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Product Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Top Product Categories ---\")\n",
    "print(trans_df['Product_Category'].value_counts())\n",
    "\n",
    "print(\"\\n--- Revenue by Product Category ---\")\n",
    "category_revenue = trans_df.groupby('Product_Category')['Revenue'].sum().sort_values(ascending=False)\n",
    "print(category_revenue)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Category counts\n",
    "trans_df['Product_Category'].value_counts().head(10).plot(kind='barh', ax=axes[0], color='teal')\n",
    "axes[0].set_title('Top 10 Product Categories by Transaction Count', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Number of Transactions')\n",
    "\n",
    "# Category revenue\n",
    "category_revenue.head(10).plot(kind='barh', ax=axes[1], color='coral')\n",
    "axes[1].set_title('Top 10 Product Categories by Revenue', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Total Revenue (₦)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Churn Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Churn Analysis ---\")\n",
    "churned_customers = rfm_df[rfm_df['Recency'] > 90]\n",
    "print(f\"Total Customers: {len(rfm_df):,}\")\n",
    "print(f\"At Risk/Churned (>90 days): {len(churned_customers):,} ({len(churned_customers)/len(rfm_df)*100:.1f}%)\")\n",
    "print(f\"Revenue from Churned Customers: ₦{churned_customers['Monetary'].sum():,.2f}\")\n",
    "print(f\"Active Customers (<=30 days): {len(rfm_df[rfm_df['Recency'] <= 30]):,} ({len(rfm_df[rfm_df['Recency'] <= 30])/len(rfm_df)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Top Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Top 10 Customers by Revenue ---\")\n",
    "top_customers = rfm_df.nlargest(10, 'Monetary')[['Customer_ID', 'Monetary', 'Frequency', 'Recency']]\n",
    "display(top_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Comprehensive Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive EDA dashboard\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "fig.suptitle('AFRIMASH CUSTOMER INTELLIGENCE - EXPLORATORY DATA ANALYSIS', fontsize=20, fontweight='bold')\n",
    "\n",
    "# 1. Frequency Distribution\n",
    "axes[0, 0].hist(rfm_df['Frequency'], bins=50, color='skyblue', edgecolor='black')\n",
    "axes[0, 0].set_title('Frequency Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Number of Purchases')\n",
    "axes[0, 0].set_ylabel('Number of Customers')\n",
    "axes[0, 0].axvline(rfm_df['Frequency'].median(), color='red', linestyle='--', \n",
    "                   label=f'Median: {rfm_df[\"Frequency\"].median():.0f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# 2. Monetary Distribution (log scale)\n",
    "axes[0, 1].hist(np.log10(rfm_df['Monetary'][rfm_df['Monetary'] > 0]), bins=50, \n",
    "                color='lightgreen', edgecolor='black')\n",
    "axes[0, 1].set_title('Monetary Value Distribution (Log Scale)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Log10(Total Spent)')\n",
    "axes[0, 1].set_ylabel('Number of Customers')\n",
    "\n",
    "# 3. Recency Distribution\n",
    "axes[0, 2].hist(rfm_df['Recency'], bins=50, color='lightcoral', edgecolor='black')\n",
    "axes[0, 2].set_title('Recency Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Days Since Last Purchase')\n",
    "axes[0, 2].set_ylabel('Number of Customers')\n",
    "axes[0, 2].axvline(90, color='red', linestyle='--', label='90 days (Churn threshold)')\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# 4. Customer Type\n",
    "customer_type_counts = rfm_df['Customer_Type'].value_counts()\n",
    "axes[1, 0].pie(customer_type_counts, labels=customer_type_counts.index, autopct='%1.1f%%', \n",
    "               colors=['#ff9999', '#66b3ff'], startangle=90)\n",
    "axes[1, 0].set_title('Customer Type Distribution', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 5. Frequency Category\n",
    "freq_cat = rfm_df['Frequency_Category'].value_counts()\n",
    "axes[1, 1].bar(freq_cat.index, freq_cat.values, color='purple', alpha=0.7)\n",
    "axes[1, 1].set_title('Frequency Categories', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Category')\n",
    "axes[1, 1].set_ylabel('Number of Customers')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Recency Category\n",
    "rec_cat = rfm_df['Recency_Category'].value_counts()\n",
    "axes[1, 2].barh(rec_cat.index, rec_cat.values, color='orange', alpha=0.7)\n",
    "axes[1, 2].set_title('Recency Categories', fontsize=14, fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Number of Customers')\n",
    "\n",
    "# 7. Product Categories\n",
    "prod_cat = trans_df['Product_Category'].value_counts().head(8)\n",
    "axes[2, 0].bar(prod_cat.index, prod_cat.values, color='teal', alpha=0.7)\n",
    "axes[2, 0].set_title('Top Product Categories', fontsize=14, fontweight='bold')\n",
    "axes[2, 0].set_xlabel('Category')\n",
    "axes[2, 0].set_ylabel('Number of Transactions')\n",
    "axes[2, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 8. Monthly Revenue Trend\n",
    "monthly_revenue = trans_df.groupby(['Year', 'Month'])['Revenue'].sum().reset_index()\n",
    "monthly_revenue['YearMonth'] = monthly_revenue['Year'].astype(str) + '-' + monthly_revenue['Month'].astype(str).str.zfill(2)\n",
    "recent_months = monthly_revenue.tail(24)\n",
    "axes[2, 1].plot(range(len(recent_months)), recent_months['Revenue'], marker='o', color='green', linewidth=2)\n",
    "axes[2, 1].set_title('Monthly Revenue Trend (Last 24 Months)', fontsize=14, fontweight='bold')\n",
    "axes[2, 1].set_xlabel('Month')\n",
    "axes[2, 1].set_ylabel('Revenue (₦)')\n",
    "axes[2, 1].tick_params(axis='x', rotation=45)\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 9. Revenue by Customer Type\n",
    "customer_type_revenue = rfm_df.groupby('Customer_Type')['Monetary'].sum()\n",
    "axes[2, 2].bar(customer_type_revenue.index, customer_type_revenue.values, color=['#ff9999', '#66b3ff'])\n",
    "axes[2, 2].set_title('Total Revenue by Customer Type', fontsize=14, fontweight='bold')\n",
    "axes[2, 2].set_xlabel('Customer Type')\n",
    "axes[2, 2].set_ylabel('Total Revenue (₦)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eda_dashboard.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: eda_dashboard.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Deep Dive Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('AFRIMASH DEEP DIVE ANALYSIS', fontsize=18, fontweight='bold')\n",
    "\n",
    "# 1. RFM Scatter Plot\n",
    "scatter = axes[0, 0].scatter(rfm_df['Recency'], rfm_df['Frequency'], \n",
    "                            c=rfm_df['Monetary'], cmap='viridis', \n",
    "                            alpha=0.6, s=50)\n",
    "axes[0, 0].set_title('RFM Analysis: Recency vs Frequency', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Recency (Days)')\n",
    "axes[0, 0].set_ylabel('Frequency (Purchases)')\n",
    "axes[0, 0].axvline(90, color='red', linestyle='--', alpha=0.5, label='90-day threshold')\n",
    "axes[0, 0].legend()\n",
    "plt.colorbar(scatter, ax=axes[0, 0], label='Monetary Value (₦)')\n",
    "\n",
    "# 2. Revenue by Category\n",
    "cat_rev = trans_df.groupby('Product_Category')['Revenue'].sum().sort_values(ascending=True).tail(10)\n",
    "axes[0, 1].barh(cat_rev.index, cat_rev.values, color='coral')\n",
    "axes[0, 1].set_title('Revenue by Product Category (Top 10)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Total Revenue (₦)')\n",
    "\n",
    "# 3. Customer Lifetime Distribution\n",
    "axes[1, 0].hist(rfm_df['Customer_Age_Days'], bins=50, color='mediumpurple', edgecolor='black')\n",
    "axes[1, 0].set_title('Customer Lifetime Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Days Since First Purchase')\n",
    "axes[1, 0].set_ylabel('Number of Customers')\n",
    "axes[1, 0].axvline(rfm_df['Customer_Age_Days'].median(), color='red', \n",
    "                   linestyle='--', label=f'Median: {rfm_df[\"Customer_Age_Days\"].median():.0f} days')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# 4. Day of Week Analysis\n",
    "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "day_trans = trans_df['DayName'].value_counts().reindex(day_order)\n",
    "axes[1, 1].bar(day_trans.index, day_trans.values, color='steelblue', alpha=0.7)\n",
    "axes[1, 1].set_title('Transactions by Day of Week', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Day')\n",
    "axes[1, 1].set_ylabel('Number of Transactions')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('deep_dive_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: deep_dive_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 💾 PART 7: Save Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned datasets\n",
    "rfm_df.to_csv('rfm_clean.csv', index=False)\n",
    "trans_df.to_csv('transactions_clean.csv', index=False)\n",
    "\n",
    "print(\"✓ Saved: rfm_clean.csv\")\n",
    "print(\"✓ Saved: transactions_clean.csv\")\n",
    "\n",
    "# Save summary statistics\n",
    "summary_stats = {\n",
    "    'Metric': [\n",
    "        'Total Customers',\n",
    "        'Total Transactions',\n",
    "        'Total Revenue',\n",
    "        'Average Customer Value',\n",
    "        'Average Transaction Value',\n",
    "        'Median Frequency',\n",
    "        'Median Recency (days)',\n",
    "        'Active Customers (0-30 days)',\n",
    "        'At Risk Customers (>90 days)',\n",
    "        'One-time Buyers',\n",
    "        'Returning Customers Rate',\n",
    "        'Average Purchase Frequency',\n",
    "        'Most Popular Category'\n",
    "    ],\n",
    "    'Value': [\n",
    "        f\"{len(rfm_df):,}\",\n",
    "        f\"{len(trans_df):,}\",\n",
    "        f\"₦{trans_df['Revenue'].sum():,.2f}\",\n",
    "        f\"₦{rfm_df['Monetary'].mean():,.2f}\",\n",
    "        f\"₦{trans_df['Revenue'].mean():,.2f}\",\n",
    "        f\"{rfm_df['Frequency'].median():.0f}\",\n",
    "        f\"{rfm_df['Recency'].median():.0f}\",\n",
    "        f\"{len(rfm_df[rfm_df['Recency'] <= 30]):,} ({len(rfm_df[rfm_df['Recency'] <= 30])/len(rfm_df)*100:.1f}%)\",\n",
    "        f\"{len(rfm_df[rfm_df['Recency'] > 90]):,} ({len(rfm_df[rfm_df['Recency'] > 90])/len(rfm_df)*100:.1f}%)\",\n",
    "        f\"{len(rfm_df[rfm_df['Frequency'] == 1]):,} ({len(rfm_df[rfm_df['Frequency'] == 1])/len(rfm_df)*100:.1f}%)\",\n",
    "        f\"{(rfm_df['Customer_Type']=='returning').sum()/len(rfm_df)*100:.1f}%\",\n",
    "        f\"{rfm_df['Frequency'].mean():.2f}\",\n",
    "        trans_df['Product_Category'].mode()[0]\n",
    "    ]\n",
    "}\n",
    "\n",
    "summary_df = pd.DataFrame(summary_stats)\n",
    "summary_df.to_csv('summary_statistics.csv', index=False)\n",
    "print(\"✓ Saved: summary_statistics.csv\")\n",
    "\n",
    "print(\"\\n📊 KEY INSIGHTS SUMMARY:\")\n",
    "print(\"=\"*80)\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 👥 PART 8: Customer Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 RFM Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"RFM SEGMENTATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate RFM Scores (1-5 scale)\n",
    "rfm_df['R_Score'] = pd.qcut(rfm_df['Recency'], q=5, labels=[5,4,3,2,1], duplicates='drop')\n",
    "rfm_df['F_Score'] = pd.qcut(rfm_df['Frequency'].rank(method='first'), q=5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "rfm_df['M_Score'] = pd.qcut(rfm_df['Monetary'], q=5, labels=[1,2,3,4,5], duplicates='drop')\n",
    "\n",
    "# Convert to numeric\n",
    "rfm_df['R_Score'] = rfm_df['R_Score'].astype(int)\n",
    "rfm_df['F_Score'] = rfm_df['F_Score'].astype(int)\n",
    "rfm_df['M_Score'] = rfm_df['M_Score'].astype(int)\n",
    "\n",
    "# Calculate overall RFM Score\n",
    "rfm_df['RFM_Score'] = rfm_df['R_Score'] + rfm_df['F_Score'] + rfm_df['M_Score']\n",
    "\n",
    "print(f\"✓ Calculated RFM Scores (Range: {rfm_df['RFM_Score'].min()}-{rfm_df['RFM_Score'].max()})\")\n",
    "\n",
    "# Define RFM Segments\n",
    "def get_rfm_segment(row):\n",
    "    r, f, m = row['R_Score'], row['F_Score'], row['M_Score']\n",
    "    \n",
    "    if r >= 4 and f >= 4 and m >= 4:\n",
    "        return 'Champions'\n",
    "    elif r >= 3 and f >= 4:\n",
    "        return 'Loyal Customers'\n",
    "    elif m >= 4 and f <= 3:\n",
    "        return 'Big Spenders'\n",
    "    elif r >= 4 and f <= 2:\n",
    "        return 'Promising'\n",
    "    elif r >= 3 and f >= 2 and m >= 2:\n",
    "        return 'Needs Attention'\n",
    "    elif r == 2 or r == 3:\n",
    "        return 'About to Sleep'\n",
    "    elif r <= 2 and f >= 3 and m >= 3:\n",
    "        return 'At Risk'\n",
    "    elif f >= 4 and m >= 4:\n",
    "        return \"Can't Lose Them\"\n",
    "    elif r <= 2 and f <= 2 and m >= 2:\n",
    "        return 'Hibernating'\n",
    "    else:\n",
    "        return 'Lost'\n",
    "\n",
    "rfm_df['RFM_Segment'] = rfm_df.apply(get_rfm_segment, axis=1)\n",
    "\n",
    "print(f\"✓ Created {rfm_df['RFM_Segment'].nunique()} RFM segments\\n\")\n",
    "print(\"Segment Distribution:\")\n",
    "print(rfm_df['RFM_Segment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"K-MEANS CLUSTERING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Select features for clustering\n",
    "clustering_features = ['Recency', 'Frequency', 'Monetary', 'Avg_Order_Value',\n",
    "                       'Customer_Age_Days', 'Purchase_Rate']\n",
    "\n",
    "X = rfm_df[clustering_features].fillna(0)\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply K-Means\n",
    "optimal_k = 5\n",
    "print(f\"\\nUsing {optimal_k} clusters...\")\n",
    "\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "rfm_df['Cluster'] = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Name clusters\n",
    "def name_cluster(cluster_id, cluster_data):\n",
    "    avg_recency = cluster_data['Recency'].mean()\n",
    "    avg_frequency = cluster_data['Frequency'].mean()\n",
    "    avg_monetary = cluster_data['Monetary'].mean()\n",
    "    \n",
    "    if avg_monetary > rfm_df['Monetary'].quantile(0.75) and avg_frequency > 10 and avg_recency < 180:\n",
    "        return 'Loyal High-Value'\n",
    "    elif avg_monetary > rfm_df['Monetary'].quantile(0.5) and avg_recency > 365:\n",
    "        return 'Dormant Veterans'\n",
    "    elif avg_monetary < rfm_df['Monetary'].quantile(0.25) and avg_recency > 365:\n",
    "        return 'Lost Low-Value'\n",
    "    elif avg_recency > 180 and avg_frequency < 10:\n",
    "        return 'Mid-Tier Customers'\n",
    "    elif avg_recency < 90:\n",
    "        return 'Recent Engagers'\n",
    "    return f'Cluster_{cluster_id}'\n",
    "\n",
    "cluster_names = {}\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = rfm_df[rfm_df['Cluster'] == cluster_id]\n",
    "    cluster_names[cluster_id] = name_cluster(cluster_id, cluster_data)\n",
    "\n",
    "rfm_df['Cluster_Name'] = rfm_df['Cluster'].map(cluster_names)\n",
    "\n",
    "print(f\"✓ Named {optimal_k} clusters\\n\")\n",
    "print(\"Cluster Distribution:\")\n",
    "print(rfm_df['Cluster_Name'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3 Segment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM Segment Summary\n",
    "rfm_segment_summary = rfm_df.groupby('RFM_Segment').agg({\n",
    "    'Customer_ID': 'count',\n",
    "    'Monetary': ['sum', 'mean'],\n",
    "    'Frequency': 'mean',\n",
    "    'Recency': 'mean',\n",
    "    'Avg_Order_Value': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "rfm_segment_summary.columns = ['_'.join(col).strip('_') for col in rfm_segment_summary.columns]\n",
    "rfm_segment_summary = rfm_segment_summary.reset_index()\n",
    "rfm_segment_summary = rfm_segment_summary.sort_values('Monetary_sum', ascending=False)\n",
    "\n",
    "print(\"\\n📊 RFM Segment Summary:\")\n",
    "display(rfm_segment_summary[['RFM_Segment', 'Customer_ID_count', 'Monetary_sum', 'Monetary_mean']])\n",
    "\n",
    "# Cluster Summary\n",
    "cluster_summary = rfm_df.groupby('Cluster_Name').agg({\n",
    "    'Customer_ID': 'count',\n",
    "    'Monetary': ['sum', 'mean'],\n",
    "    'Frequency': 'mean',\n",
    "    'Recency': 'mean',\n",
    "    'Avg_Order_Value': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "cluster_summary.columns = ['_'.join(col).strip('_') for col in cluster_summary.columns]\n",
    "cluster_summary = cluster_summary.reset_index()\n",
    "cluster_summary = cluster_summary.sort_values('Monetary_sum', ascending=False)\n",
    "\n",
    "print(\"\\n🤖 K-Means Cluster Summary:\")\n",
    "display(cluster_summary[['Cluster_Name', 'Customer_ID_count', 'Monetary_sum', 'Monetary_mean']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4 Save Segmentation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save segmented data\n",
    "rfm_df.to_csv('rfm_segmented.csv', index=False)\n",
    "rfm_segment_summary.to_csv('rfm_segment_summary.csv', index=False)\n",
    "cluster_summary.to_csv('cluster_summary.csv', index=False)\n",
    "\n",
    "print(\"✓ Saved: rfm_segmented.csv\")\n",
    "print(\"✓ Saved: rfm_segment_summary.csv\")\n",
    "print(\"✓ Saved: cluster_summary.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5 Segmentation Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
    "fig.suptitle('CUSTOMER SEGMENTATION ANALYSIS', fontsize=18, fontweight='bold')\n",
    "\n",
    "# 1. RFM Segment Distribution\n",
    "segment_counts = rfm_df['RFM_Segment'].value_counts()\n",
    "colors = plt.cm.Set3(range(len(segment_counts)))\n",
    "segment_counts.plot(kind='barh', ax=axes[0, 0], color=colors)\n",
    "axes[0, 0].set_title('Customer Distribution by RFM Segment', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Number of Customers')\n",
    "\n",
    "# 2. Revenue by RFM Segment\n",
    "segment_revenue = rfm_df.groupby('RFM_Segment')['Monetary'].sum().sort_values(ascending=False)\n",
    "segment_revenue_billions = segment_revenue / 1e9\n",
    "segment_revenue_billions.plot(kind='barh', ax=axes[0, 1], color=colors)\n",
    "axes[0, 1].set_title('Total Revenue by RFM Segment (₦ Billions)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Revenue (₦B)')\n",
    "\n",
    "# 3. Cluster Distribution\n",
    "cluster_counts = rfm_df['Cluster_Name'].value_counts()\n",
    "colors2 = plt.cm.Pastel1(range(len(cluster_counts)))\n",
    "cluster_counts.plot(kind='barh', ax=axes[1, 0], color=colors2)\n",
    "axes[1, 0].set_title('Customer Distribution by K-Means Cluster', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Number of Customers')\n",
    "\n",
    "# 4. Revenue by Cluster\n",
    "cluster_revenue = rfm_df.groupby('Cluster_Name')['Monetary'].sum().sort_values(ascending=False)\n",
    "cluster_revenue_billions = cluster_revenue / 1e9\n",
    "cluster_revenue_billions.plot(kind='barh', ax=axes[1, 1], color=colors2)\n",
    "axes[1, 1].set_title('Total Revenue by Cluster (₦ Billions)', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Revenue (₦B)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('rfm_segmentation.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: rfm_segmentation.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔮 PART 9: Predictive Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Churn Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"CHURN PREDICTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Define churn\n",
    "CHURN_THRESHOLD = 90\n",
    "rfm_df['Is_Churned'] = (rfm_df['Recency'] > CHURN_THRESHOLD).astype(int)\n",
    "\n",
    "print(f\"\\n✓ Defined churn threshold: {CHURN_THRESHOLD} days\")\n",
    "print(f\"  Churned customers: {rfm_df['Is_Churned'].sum():,} ({rfm_df['Is_Churned'].mean()*100:.1f}%)\")\n",
    "print(f\"  Active customers: {(1-rfm_df['Is_Churned']).sum():,} ({(1-rfm_df['Is_Churned'].mean())*100:.1f}%)\")\n",
    "\n",
    "# Prepare features\n",
    "churn_features = ['Frequency', 'Monetary', 'Avg_Order_Value', 'Customer_Age_Days',\n",
    "                  'Purchase_Rate', 'Recency', 'Total_Items_Sold']\n",
    "\n",
    "category_cols = [col for col in rfm_df.columns if col.startswith('Category_')]\n",
    "churn_features.extend(category_cols)\n",
    "\n",
    "if 'R_Score' in rfm_df.columns:\n",
    "    churn_features.extend(['R_Score', 'F_Score', 'M_Score'])\n",
    "\n",
    "if 'Customer_Type' in rfm_df.columns:\n",
    "    le_customer = LabelEncoder()\n",
    "    rfm_df['Customer_Type_Encoded'] = le_customer.fit_transform(rfm_df['Customer_Type'])\n",
    "    churn_features.append('Customer_Type_Encoded')\n",
    "\n",
    "X_churn = rfm_df[churn_features].fillna(0)\n",
    "y_churn = rfm_df['Is_Churned']\n",
    "\n",
    "# Train-test split\n",
    "X_train_churn, X_test_churn, y_train_churn, y_test_churn = train_test_split(\n",
    "    X_churn, y_churn, test_size=0.2, random_state=42, stratify=y_churn\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training set: {len(X_train_churn):,} samples\")\n",
    "print(f\"✓ Test set: {len(X_test_churn):,} samples\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\n🤖 Training Gradient Boosting Classifier...\")\n",
    "churn_model = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    subsample=0.8\n",
    ")\n",
    "\n",
    "churn_model.fit(X_train_churn, y_train_churn)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_churn = churn_model.predict(X_test_churn)\n",
    "y_pred_proba_churn = churn_model.predict_proba(X_test_churn)[:, 1]\n",
    "\n",
    "# Evaluate\n",
    "accuracy = accuracy_score(y_test_churn, y_pred_churn)\n",
    "auc_roc = roc_auc_score(y_test_churn, y_pred_proba_churn)\n",
    "\n",
    "print(f\"\\n✓ Churn Model Performance:\")\n",
    "print(f\"  Accuracy: {accuracy*100:.1f}%\")\n",
    "print(f\"  AUC-ROC: {auc_roc:.3f}\")\n",
    "\n",
    "# Predict for all customers\n",
    "rfm_df['Churn_Probability'] = churn_model.predict_proba(X_churn)[:, 1]\n",
    "\n",
    "# Categorize churn risk\n",
    "def categorize_churn_risk(prob):\n",
    "    if prob < 0.3:\n",
    "        return 'Low'\n",
    "    elif prob < 0.5:\n",
    "        return 'Medium'\n",
    "    elif prob < 0.7:\n",
    "        return 'High'\n",
    "    else:\n",
    "        return 'Critical'\n",
    "\n",
    "rfm_df['Churn_Risk_Level'] = rfm_df['Churn_Probability'].apply(categorize_churn_risk)\n",
    "\n",
    "print(f\"\\n✓ Churn Risk Distribution:\")\n",
    "print(rfm_df['Churn_Risk_Level'].value_counts())\n",
    "\n",
    "# Save model\n",
    "with open('churn_model.pkl', 'wb') as f:\n",
    "    pickle.dump(churn_model, f)\n",
    "print(\"\\n✓ Saved churn_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 Customer Lifetime Value (CLV) Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLV PREDICTION MODEL\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare features\n",
    "clv_features = ['Frequency', 'Avg_Order_Value', 'Customer_Age_Days',\n",
    "                'Purchase_Rate', 'Recency', 'Total_Items_Sold']\n",
    "\n",
    "clv_features.extend(category_cols)\n",
    "\n",
    "if 'R_Score' in rfm_df.columns:\n",
    "    clv_features.extend(['R_Score', 'F_Score', 'M_Score'])\n",
    "\n",
    "if 'Customer_Type_Encoded' in rfm_df.columns:\n",
    "    clv_features.append('Customer_Type_Encoded')\n",
    "\n",
    "clv_data = rfm_df[rfm_df['Monetary'] > 0].copy()\n",
    "\n",
    "X_clv = clv_data[clv_features].fillna(0)\n",
    "y_clv = clv_data['Monetary']\n",
    "\n",
    "# Train-test split\n",
    "X_train_clv, X_test_clv, y_train_clv, y_test_clv = train_test_split(\n",
    "    X_clv, y_clv, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Training set: {len(X_train_clv):,} samples\")\n",
    "print(f\"✓ Test set: {len(X_test_clv):,} samples\")\n",
    "\n",
    "# Train model\n",
    "print(\"\\n🤖 Training Gradient Boosting Regressor...\")\n",
    "clv_model = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=5,\n",
    "    random_state=42,\n",
    "    subsample=0.8\n",
    ")\n",
    "\n",
    "clv_model.fit(X_train_clv, y_train_clv)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_clv = clv_model.predict(X_test_clv)\n",
    "\n",
    "# Evaluate\n",
    "r2 = r2_score(y_test_clv, y_pred_clv)\n",
    "mae = mean_absolute_error(y_test_clv, y_pred_clv)\n",
    "rmse = np.sqrt(mean_squared_error(y_test_clv, y_pred_clv))\n",
    "\n",
    "print(f\"\\n✓ CLV Model Performance:\")\n",
    "print(f\"  R² Score: {r2:.3f} ({r2*100:.1f}% variance explained)\")\n",
    "print(f\"  MAE: ₦{mae:,.0f}\")\n",
    "print(f\"  RMSE: ₦{rmse:,.0f}\")\n",
    "\n",
    "# Predict for all customers\n",
    "X_all_clv = rfm_df[clv_features].fillna(0)\n",
    "rfm_df['Predicted_CLV'] = clv_model.predict(X_all_clv)\n",
    "rfm_df['Predicted_CLV'] = rfm_df['Predicted_CLV'].clip(lower=0)\n",
    "\n",
    "# Categorize CLV\n",
    "clv_percentiles = rfm_df['Predicted_CLV'].quantile([0.25, 0.50, 0.75, 0.90])\n",
    "\n",
    "def categorize_clv(clv):\n",
    "    if clv >= clv_percentiles[0.90]:\n",
    "        return 'Very High Value'\n",
    "    elif clv >= clv_percentiles[0.75]:\n",
    "        return 'High Value'\n",
    "    elif clv >= clv_percentiles[0.50]:\n",
    "        return 'Medium Value'\n",
    "    elif clv >= clv_percentiles[0.25]:\n",
    "        return 'Low Value'\n",
    "    else:\n",
    "        return 'Very Low Value'\n",
    "\n",
    "rfm_df['CLV_Category'] = rfm_df['Predicted_CLV'].apply(categorize_clv)\n",
    "\n",
    "print(f\"\\n✓ CLV Distribution:\")\n",
    "print(rfm_df['CLV_Category'].value_counts())\n",
    "print(f\"\\n  Total Predicted CLV: ₦{rfm_df['Predicted_CLV'].sum()/1e9:.2f}B\")\n",
    "print(f\"  Average CLV: ₦{rfm_df['Predicted_CLV'].mean()/1e6:.2f}M\")\n",
    "\n",
    "# Save model\n",
    "with open('clv_model.pkl', 'wb') as f:\n",
    "    pickle.dump(clv_model, f)\n",
    "print(\"\\n✓ Saved clv_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 Purchase Timing Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PURCHASE TIMING ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate expected days to next purchase\n",
    "rfm_df['Days_Between_Purchases'] = rfm_df['Customer_Age_Days'] / rfm_df['Frequency'].replace(0, 1)\n",
    "rfm_df['Expected_Days_to_Next_Purchase'] = rfm_df['Days_Between_Purchases']\n",
    "\n",
    "rfm_df['Days_Since_Last_Purchase'] = rfm_df['Recency']\n",
    "rfm_df['Days_Overdue'] = (rfm_df['Days_Since_Last_Purchase'] -\n",
    "                          rfm_df['Expected_Days_to_Next_Purchase']).clip(lower=0)\n",
    "\n",
    "# Categorize purchase timing\n",
    "def categorize_purchase_timing(row):\n",
    "    days_since = row['Days_Since_Last_Purchase']\n",
    "    expected_days = row['Expected_Days_to_Next_Purchase']\n",
    "    \n",
    "    if row['Frequency'] == 1:\n",
    "        return 'New/One-time'\n",
    "    elif days_since < expected_days * 0.8:\n",
    "        return 'Due Soon'\n",
    "    elif days_since < expected_days * 1.2:\n",
    "        return 'On Track'\n",
    "    elif days_since < expected_days * 2:\n",
    "        return 'Slightly Overdue'\n",
    "    elif days_since < expected_days * 3:\n",
    "        return 'Overdue'\n",
    "    else:\n",
    "        return 'Severely Overdue'\n",
    "\n",
    "rfm_df['Purchase_Timing_Status'] = rfm_df.apply(categorize_purchase_timing, axis=1)\n",
    "\n",
    "print(f\"\\n✓ Purchase Timing Distribution:\")\n",
    "print(rfm_df['Purchase_Timing_Status'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4 Customer Priority Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CUSTOMER PRIORITY SCORING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate customer value score\n",
    "rfm_df['Customer_Value_Score'] = (\n",
    "    (rfm_df['Predicted_CLV'] / rfm_df['Predicted_CLV'].max() * 50) +\n",
    "    (rfm_df['Churn_Probability'] * 50)\n",
    ")\n",
    "\n",
    "# Categorize priority\n",
    "def categorize_priority(row):\n",
    "    clv = row['Predicted_CLV']\n",
    "    churn_prob = row['Churn_Probability']\n",
    "    \n",
    "    high_clv = clv > rfm_df['Predicted_CLV'].quantile(0.75)\n",
    "    high_churn = churn_prob > 0.5\n",
    "    \n",
    "    if high_clv and high_churn:\n",
    "        return 'CRITICAL'\n",
    "    elif high_clv:\n",
    "        return 'High'\n",
    "    elif high_churn and clv > rfm_df['Predicted_CLV'].quantile(0.5):\n",
    "        return 'Medium'\n",
    "    else:\n",
    "        return 'Low'\n",
    "\n",
    "rfm_df['Customer_Priority'] = rfm_df.apply(categorize_priority, axis=1)\n",
    "\n",
    "print(f\"\\n✓ Customer Priority Distribution:\")\n",
    "print(rfm_df['Customer_Priority'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5 Save Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete dataset with predictions\n",
    "rfm_df.to_csv('rfm_with_predictions.csv', index=False)\n",
    "print(\"✓ Saved: rfm_with_predictions.csv\")\n",
    "\n",
    "# Save high-risk customers\n",
    "high_risk = rfm_df[\n",
    "    (rfm_df['Churn_Risk_Level'].isin(['High', 'Critical'])) &\n",
    "    (rfm_df['Monetary'] > rfm_df['Monetary'].quantile(0.5))\n",
    "].sort_values('Monetary', ascending=False)\n",
    "\n",
    "high_risk[['Customer_ID', 'RFM_Segment', 'Monetary', 'Predicted_CLV',\n",
    "           'Churn_Probability', 'Churn_Risk_Level', 'Customer_Priority']].to_csv(\n",
    "    'high_risk_customers.csv', index=False\n",
    ")\n",
    "print(f\"✓ Saved: high_risk_customers.csv ({len(high_risk):,} customers)\")\n",
    "\n",
    "# Save high-value opportunities\n",
    "high_value = rfm_df[\n",
    "    rfm_df['Predicted_CLV'] > rfm_df['Predicted_CLV'].quantile(0.90)\n",
    "].sort_values('Predicted_CLV', ascending=False)\n",
    "\n",
    "high_value[['Customer_ID', 'RFM_Segment', 'Monetary', 'Predicted_CLV',\n",
    "            'Churn_Probability', 'CLV_Category']].to_csv(\n",
    "    'high_value_opportunities.csv', index=False\n",
    ")\n",
    "print(f\"✓ Saved: high_value_opportunities.csv ({len(high_value):,} customers)\")\n",
    "\n",
    "# Save action priority list\n",
    "action_priority = rfm_df.sort_values('Customer_Value_Score', ascending=False)\n",
    "action_priority[['Customer_ID', 'RFM_Segment', 'Monetary', 'Predicted_CLV',\n",
    "                 'Churn_Probability', 'Purchase_Timing_Status', \n",
    "                 'Customer_Priority', 'Customer_Value_Score']].to_csv(\n",
    "    'action_priority_list.csv', index=False\n",
    ")\n",
    "print(f\"✓ Saved: action_priority_list.csv\")\n",
    "\n",
    "# Save model summary\n",
    "model_summary = pd.DataFrame({\n",
    "    'Model': ['Churn Prediction', 'CLV Prediction'],\n",
    "    'Algorithm': ['Gradient Boosting Classifier', 'Gradient Boosting Regressor'],\n",
    "    'Performance': [f'{auc_roc:.3f} (AUC-ROC)', f'{r2:.3f} (R²)'],\n",
    "    'Training_Samples': [len(X_train_churn), len(X_train_clv)],\n",
    "    'Features_Used': [len(churn_features), len(clv_features)]\n",
    "})\n",
    "model_summary.to_csv('model_summary.csv', index=False)\n",
    "print(f\"✓ Saved: model_summary.csv\")\n",
    "\n",
    "print(\"\\n✅ All prediction models complete and saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.6 Prediction Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Churn Analysis Visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('CHURN & CLV PREDICTION ANALYSIS', fontsize=18, fontweight='bold')\n",
    "\n",
    "# 1. Churn Risk Distribution\n",
    "risk_counts = rfm_df['Churn_Risk_Level'].value_counts()\n",
    "risk_order = ['Low', 'Medium', 'High', 'Critical']\n",
    "risk_counts = risk_counts.reindex(risk_order, fill_value=0)\n",
    "risk_colors = {'Low': '#28a745', 'Medium': '#ffc107', 'High': '#fd7e14', 'Critical': '#dc3545'}\n",
    "colors = [risk_colors[x] for x in risk_order]\n",
    "axes[0, 0].bar(risk_order, risk_counts.values, color=colors)\n",
    "axes[0, 0].set_title('Churn Risk Level Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Number of Customers')\n",
    "\n",
    "# 2. ROC Curve\n",
    "fpr, tpr, _ = roc_curve(y_test_churn, y_pred_proba_churn)\n",
    "axes[0, 1].plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {auc_roc:.3f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random')\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ROC Curve - Churn Prediction', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# 3. CLV Distribution\n",
    "axes[1, 0].hist(rfm_df['Predicted_CLV'], bins=50, color='green', edgecolor='black', alpha=0.7)\n",
    "axes[1, 0].set_xlabel('Predicted CLV (₦)')\n",
    "axes[1, 0].set_ylabel('Number of Customers')\n",
    "axes[1, 0].set_title('Predicted CLV Distribution', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].set_xscale('log')\n",
    "\n",
    "# 4. CLV vs Churn Matrix\n",
    "scatter = axes[1, 1].scatter(rfm_df['Churn_Probability'], rfm_df['Predicted_CLV'],\n",
    "                             c=rfm_df['Customer_Value_Score'], cmap='RdYlGn_r',\n",
    "                             alpha=0.6, s=30)\n",
    "axes[1, 1].set_xlabel('Churn Probability')\n",
    "axes[1, 1].set_ylabel('Predicted CLV (₦)')\n",
    "axes[1, 1].set_title('CLV vs Churn Risk Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].set_yscale('log')\n",
    "axes[1, 1].axvline(0.5, color='red', linestyle='--', linewidth=1, alpha=0.5)\n",
    "plt.colorbar(scatter, ax=axes[1, 1], label='Priority Score')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('churn_prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: churn_prediction_analysis.png\")\n",
    "plt.savefig('clv_prediction_analysis.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: clv_prediction_analysis.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer Priority Matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.suptitle('CUSTOMER PRIORITY MATRIX', fontsize=18, fontweight='bold')\n",
    "\n",
    "# Priority Distribution\n",
    "priority_counts = rfm_df['Customer_Priority'].value_counts()\n",
    "colors_dict = {'CRITICAL': '#dc3545', 'High': '#fd7e14', 'Medium': '#ffc107', 'Low': '#28a745'}\n",
    "priority_colors = [colors_dict.get(p, '#6c757d') for p in priority_counts.index]\n",
    "priority_counts.plot(kind='bar', ax=axes[0], color=priority_colors)\n",
    "axes[0].set_title('Customer Priority Distribution', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylabel('Number of Customers')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Priority by Revenue\n",
    "priority_revenue = rfm_df.groupby('Customer_Priority')['Monetary'].sum() / 1e9\n",
    "priority_revenue.plot(kind='bar', ax=axes[1], color=priority_colors)\n",
    "axes[1].set_title('Total Revenue by Priority (₦ Billions)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_ylabel('Revenue (₦B)')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('customer_priority_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(\"✓ Saved: customer_priority_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 PART 10: Product Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1 Prepare Product Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"PRODUCT RECOMMENDATION ENGINE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get category columns\n",
    "category_cols = [col for col in rfm_df.columns if col.startswith('Category_')]\n",
    "print(f\"\\n✓ Found {len(category_cols)} product categories\")\n",
    "\n",
    "# Create customer-product matrix\n",
    "customer_product_matrix = rfm_df[['Customer_ID'] + category_cols].copy()\n",
    "print(f\"✓ Created customer-product matrix: {customer_product_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_customers(customer_id, customer_product_matrix, top_n=10):\n",
    "    \"\"\"Find similar customers based on purchase patterns\"\"\"\n",
    "    target_purchases = customer_product_matrix[customer_product_matrix['Customer_ID'] == customer_id].iloc[0, 1:].values\n",
    "    \n",
    "    similarities = []\n",
    "    for idx, row in customer_product_matrix.iterrows():\n",
    "        if row['Customer_ID'] == customer_id:\n",
    "            continue\n",
    "        \n",
    "        other_purchases = row[1:].values\n",
    "        intersection = np.sum(np.minimum(target_purchases, other_purchases))\n",
    "        union = np.sum(np.maximum(target_purchases, other_purchases))\n",
    "        \n",
    "        if union > 0:\n",
    "            similarity = intersection / union\n",
    "            similarities.append((row['Customer_ID'], similarity))\n",
    "    \n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    return similarities[:top_n]\n",
    "\n",
    "def recommend_products_collaborative(customer_id, customer_product_matrix, top_n=5):\n",
    "    \"\"\"Recommend products based on similar customers\"\"\"\n",
    "    similar_customers = get_similar_customers(customer_id, customer_product_matrix, top_n=20)\n",
    "    \n",
    "    if not similar_customers:\n",
    "        return []\n",
    "    \n",
    "    target_row = customer_product_matrix[customer_product_matrix['Customer_ID'] == customer_id]\n",
    "    if target_row.empty:\n",
    "        return []\n",
    "    \n",
    "    target_purchases = target_row.iloc[0, 1:].values\n",
    "    \n",
    "    recommendations = defaultdict(float)\n",
    "    \n",
    "    for similar_id, similarity in similar_customers:\n",
    "        similar_row = customer_product_matrix[customer_product_matrix['Customer_ID'] == similar_id]\n",
    "        if similar_row.empty:\n",
    "            continue\n",
    "        \n",
    "        similar_purchases = similar_row.iloc[0, 1:].values\n",
    "        \n",
    "        for i, category in enumerate(category_cols):\n",
    "            if similar_purchases[i] > 0 and target_purchases[i] == 0:\n",
    "                recommendations[category] += similarity\n",
    "    \n",
    "    sorted_recs = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    result = []\n",
    "    for category, score in sorted_recs[:top_n]:\n",
    "        category_name = category.replace('Category_', '')\n",
    "        result.append({\n",
    "            'Category': category_name,\n",
    "            'Score': score,\n",
    "            'Method': 'Collaborative Filtering'\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "print(\"✓ Collaborative filtering functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 Association Rules (Market Basket Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_association_rules(customer_product_matrix, min_support=0.01, min_confidence=0.1):\n",
    "    \"\"\"Calculate product association rules\"\"\"\n",
    "    purchase_matrix = customer_product_matrix.iloc[:, 1:].values > 0\n",
    "    category_names = [col.replace('Category_', '') for col in category_cols]\n",
    "    \n",
    "    n_customers = len(purchase_matrix)\n",
    "    \n",
    "    item_support = {}\n",
    "    for i, category in enumerate(category_names):\n",
    "        support = np.sum(purchase_matrix[:, i]) / n_customers\n",
    "        if support >= min_support:\n",
    "            item_support[category] = support\n",
    "    \n",
    "    rules = []\n",
    "    \n",
    "    for i, cat1 in enumerate(category_names):\n",
    "        if cat1 not in item_support:\n",
    "            continue\n",
    "        \n",
    "        for j, cat2 in enumerate(category_names):\n",
    "            if i >= j or cat2 not in item_support:\n",
    "                continue\n",
    "            \n",
    "            both = np.sum(purchase_matrix[:, i] & purchase_matrix[:, j])\n",
    "            support_both = both / n_customers\n",
    "            \n",
    "            if support_both < min_support:\n",
    "                continue\n",
    "            \n",
    "            confidence_1_to_2 = support_both / item_support[cat1]\n",
    "            confidence_2_to_1 = support_both / item_support[cat2]\n",
    "            lift = support_both / (item_support[cat1] * item_support[cat2])\n",
    "            \n",
    "            if confidence_1_to_2 >= min_confidence:\n",
    "                rules.append({\n",
    "                    'Antecedent': cat1,\n",
    "                    'Consequent': cat2,\n",
    "                    'Support': support_both,\n",
    "                    'Confidence': confidence_1_to_2,\n",
    "                    'Lift': lift\n",
    "                })\n",
    "            \n",
    "            if confidence_2_to_1 >= min_confidence:\n",
    "                rules.append({\n",
    "                    'Antecedent': cat2,\n",
    "                    'Consequent': cat1,\n",
    "                    'Support': support_both,\n",
    "                    'Confidence': confidence_2_to_1,\n",
    "                    'Lift': lift\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(rules)\n",
    "\n",
    "# Calculate association rules\n",
    "print(\"\\nCalculating association rules...\")\n",
    "association_rules = calculate_association_rules(customer_product_matrix, min_support=0.01, min_confidence=0.1)\n",
    "association_rules = association_rules.sort_values('Lift', ascending=False)\n",
    "\n",
    "print(f\"✓ Found {len(association_rules)} association rules\")\n",
    "\n",
    "if len(association_rules) > 0:\n",
    "    print(f\"\\n📊 Top 5 Association Rules:\")\n",
    "    display(association_rules.head()[['Antecedent', 'Consequent', 'Confidence', 'Lift']])\n",
    "\n",
    "# Save\n",
    "association_rules.to_csv('product_association_rules.csv', index=False)\n",
    "print(f\"\\n✓ Saved: product_association_rules.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4 Hybrid Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_products_association(customer_id, customer_product_matrix, association_rules, top_n=5):\n",
    "    \"\"\"Recommend products based on association rules\"\"\"\n",
    "    target_row = customer_product_matrix[customer_product_matrix['Customer_ID'] == customer_id]\n",
    "    if target_row.empty:\n",
    "        return []\n",
    "    \n",
    "    target_purchases = target_row.iloc[0, 1:].values\n",
    "    purchased_categories = [col.replace('Category_', '') for i, col in enumerate(category_cols) if target_purchases[i] > 0]\n",
    "    \n",
    "    if not purchased_categories:\n",
    "        return []\n",
    "    \n",
    "    recommendations = defaultdict(float)\n",
    "    \n",
    "    for category in purchased_categories:\n",
    "        relevant_rules = association_rules[association_rules['Antecedent'] == category]\n",
    "        \n",
    "        for _, rule in relevant_rules.iterrows():\n",
    "            consequent = rule['Consequent']\n",
    "            consequent_col = f'Category_{consequent}'\n",
    "            if consequent_col in category_cols:\n",
    "                idx = category_cols.index(consequent_col)\n",
    "                if target_purchases[idx] == 0:\n",
    "                    score = rule['Confidence'] * rule['Lift']\n",
    "                    recommendations[consequent] = max(recommendations[consequent], score)\n",
    "    \n",
    "    sorted_recs = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    result = []\n",
    "    for category, score in sorted_recs[:top_n]:\n",
    "        result.append({\n",
    "            'Category': category,\n",
    "            'Score': score,\n",
    "            'Method': 'Association Rules'\n",
    "        })\n",
    "    \n",
    "    return result\n",
    "\n",
    "def get_hybrid_recommendations(customer_id, customer_product_matrix, association_rules, top_n=5):\n",
    "    \"\"\"Combine collaborative filtering and association rules\"\"\"\n",
    "    collab_recs = recommend_products_collaborative(customer_id, customer_product_matrix, top_n=10)\n",
    "    assoc_recs = recommend_products_association(customer_id, customer_product_matrix, association_rules, top_n=10)\n",
    "    \n",
    "    combined_scores = defaultdict(lambda: {'score': 0, 'methods': []})\n",
    "    \n",
    "    for rec in collab_recs:\n",
    "        category = rec['Category']\n",
    "        combined_scores[category]['score'] += rec['Score'] * 0.3\n",
    "        combined_scores[category]['methods'].append('Collaborative')\n",
    "    \n",
    "    for rec in assoc_recs:\n",
    "        category = rec['Category']\n",
    "        combined_scores[category]['score'] += rec['Score'] * 0.7\n",
    "        combined_scores[category]['methods'].append('Association')\n",
    "    \n",
    "    sorted_recs = sorted(combined_scores.items(), key=lambda x: x[1]['score'], reverse=True)\n",
    "    \n",
    "    recommendations = []\n",
    "    for category, data in sorted_recs[:top_n]:\n",
    "        normalized_score = min(1.0, data['score'] / 5.0)\n",
    "        \n",
    "        recommendations.append({\n",
    "            'Customer_ID': customer_id,\n",
    "            'Recommended_Category': category,\n",
    "            'Confidence': normalized_score,\n",
    "            'Reason': ', '.join(data['methods'])\n",
    "        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "print(\"✓ Hybrid recommendation system defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.5 Generate Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nGenerating recommendations for top customers...\")\n",
    "\n",
    "# Generate for top 1000 customers\n",
    "target_customers = rfm_df.nlargest(1000, 'Monetary')['Customer_ID'].tolist()\n",
    "print(f\"✓ Generating recommendations for {len(target_customers):,} customers...\")\n",
    "\n",
    "all_recommendations = []\n",
    "for i, customer_id in enumerate(target_customers):\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(f\"  Progress: {i+1}/{len(target_customers)} customers processed...\")\n",
    "    \n",
    "    recs = get_hybrid_recommendations(customer_id, customer_product_matrix, association_rules, top_n=5)\n",
    "    all_recommendations.extend(recs)\n",
    "\n",
    "recommendations_df = pd.DataFrame(all_recommendations)\n",
    "print(f\"\\n✓ Generated {len(recommendations_df):,} recommendations\")\n",
    "\n",
    "# Save recommendations\n",
    "recommendations_df.to_csv('product_recommendations.csv', index=False)\n",
    "print(f\"✓ Saved: product_recommendations.csv\")\n",
    "\n",
    "# Top recommendations per customer\n",
    "top_recs_per_customer = recommendations_df.sort_values(['Customer_ID', 'Confidence'], ascending=[True, False])\n",
    "top_3_per_customer = top_recs_per_customer.groupby('Customer_ID').head(3)\n",
    "top_3_per_customer.to_csv('top_recommendations_per_customer.csv', index=False)\n",
    "print(f\"✓ Saved: top_recommendations_per_customer.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.6 Cross-Sell Opportunities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nIdentifying cross-sell opportunities...\")\n",
    "\n",
    "cross_sell_opportunities = []\n",
    "\n",
    "for _, customer in rfm_df.iterrows():\n",
    "    customer_id = customer['Customer_ID']\n",
    "    \n",
    "    purchased_categories = sum([customer[col] for col in category_cols if col in customer.index])\n",
    "    \n",
    "    if purchased_categories < 3 and customer['Predicted_CLV'] > rfm_df['Predicted_CLV'].median():\n",
    "        customer_recs = recommendations_df[recommendations_df['Customer_ID'] == customer_id]\n",
    "        \n",
    "        if len(customer_recs) > 0:\n",
    "            top_rec = customer_recs.iloc[0]\n",
    "            \n",
    "            cross_sell_opportunities.append({\n",
    "                'Customer_ID': customer_id,\n",
    "                'RFM_Segment': customer.get('RFM_Segment', 'Unknown'),\n",
    "                'Current_Categories': purchased_categories,\n",
    "                'Predicted_CLV': customer['Predicted_CLV'],\n",
    "                'Recommended_Category': top_rec['Recommended_Category'],\n",
    "                'Confidence': top_rec['Confidence'],\n",
    "                'Potential_Value': customer['Predicted_CLV'] * 0.2\n",
    "            })\n",
    "\n",
    "cross_sell_df = pd.DataFrame(cross_sell_opportunities)\n",
    "cross_sell_df = cross_sell_df.sort_values('Potential_Value', ascending=False)\n",
    "\n",
    "print(f\"✓ Identified {len(cross_sell_df):,} cross-sell opportunities\")\n",
    "print(f\"  Potential Revenue: ₦{cross_sell_df['Potential_Value'].sum()/1e9:.2f}B\")\n",
    "\n",
    "cross_sell_df.to_csv('cross_sell_opportunities.csv', index=False)\n",
    "print(f\"✓ Saved: cross_sell_opportunities.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.7 Recommendation Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(recommendations_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    fig.suptitle('PRODUCT RECOMMENDATION ANALYSIS', fontsize=18, fontweight='bold')\n",
    "    \n",
    "    # 1. Top Recommended Categories\n",
    "    top_categories = recommendations_df['Recommended_Category'].value_counts().head(10)\n",
    "    colors = plt.cm.Set3(range(len(top_categories)))\n",
    "    top_categories.plot(kind='barh', ax=axes[0, 0], color=colors)\n",
    "    axes[0, 0].set_title('Top 10 Recommended Categories', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Number of Recommendations')\n",
    "    \n",
    "    # 2. Recommendation Method Distribution\n",
    "    method_dist = recommendations_df['Reason'].value_counts()\n",
    "    axes[0, 1].pie(method_dist.values, labels=method_dist.index, autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 1].set_title('Recommendation Method Distribution', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # 3. Confidence Distribution\n",
    "    axes[1, 0].hist(recommendations_df['Confidence'], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].axvline(0.7, color='red', linestyle='--', linewidth=2, label='High Confidence (0.7)')\n",
    "    axes[1, 0].set_xlabel('Confidence Score')\n",
    "    axes[1, 0].set_ylabel('Number of Recommendations')\n",
    "    axes[1, 0].set_title('Recommendation Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # 4. Cross-sell Potential\n",
    "    if len(cross_sell_df) > 0:\n",
    "        rec_categories = cross_sell_df['Recommended_Category'].value_counts().head(10)\n",
    "        colors2 = plt.cm.Paired(range(len(rec_categories)))\n",
    "        rec_categories.plot(kind='barh', ax=axes[1, 1], color=colors2)\n",
    "        axes[1, 1].set_title('Top Categories for Cross-sell', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Number of Opportunities')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('recommendation_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: recommendation_analysis.png\")\n",
    "    plt.savefig('cross_sell_opportunities.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"✓ Saved: cross_sell_opportunities.png\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No recommendations generated to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📈 PART 11: Final Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"FINAL SUMMARY - AFRIMASH CUSTOMER INTELLIGENCE SOLUTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n📊 DATA PROCESSING:\")\n",
    "print(f\"  Total Customers Analyzed: {len(rfm_df):,}\")\n",
    "print(f\"  Total Transactions: {len(trans_df):,}\")\n",
    "print(f\"  Total Revenue: ₦{trans_df['Revenue'].sum()/1e9:.2f}B\")\n",
    "print(f\"  Features Created: {rfm_df.shape[1]}\")\n",
    "\n",
    "print(\"\\n👥 CUSTOMER SEGMENTATION:\")\n",
    "print(f\"  RFM Segments: {rfm_df['RFM_Segment'].nunique()}\")\n",
    "print(f\"  K-Means Clusters: {rfm_df['Cluster_Name'].nunique()}\")\n",
    "print(f\"  Top Segment: {rfm_df['RFM_Segment'].mode()[0]}\")\n",
    "\n",
    "print(\"\\n🔮 PREDICTIVE MODELS:\")\n",
    "print(f\"  Churn Model Accuracy: {accuracy*100:.1f}%\")\n",
    "print(f\"  Churn Model AUC-ROC: {auc_roc:.3f}\")\n",
    "print(f\"  CLV Model R² Score: {r2:.3f}\")\n",
    "print(f\"  High-Risk Customers: {len(high_risk):,}\")\n",
    "print(f\"  Revenue at Risk: ₦{high_risk['Monetary'].sum()/1e9:.2f}B\")\n",
    "\n",
    "print(\"\\n🎯 RECOMMENDATIONS:\")\n",
    "if len(recommendations_df) > 0:\n",
    "    print(f\"  Total Recommendations: {len(recommendations_df):,}\")\n",
    "    print(f\"  Customers Covered: {recommendations_df['Customer_ID'].nunique():,}\")\n",
    "    print(f\"  Avg Confidence: {recommendations_df['Confidence'].mean():.2f}\")\n",
    "    print(f\"  Cross-sell Opportunities: {len(cross_sell_df):,}\")\n",
    "    print(f\"  Cross-sell Potential: ₦{cross_sell_df['Potential_Value'].sum()/1e9:.2f}B\")\n",
    "else:\n",
    "    print(\"  No recommendations generated\")\n",
    "\n",
    "print(\"\\n📁 FILES GENERATED:\")\n",
    "generated_files = [\n",
    "    'rfm_clean.csv',\n",
    "    'transactions_clean.csv',\n",
    "    'rfm_segmented.csv',\n",
    "    'rfm_with_predictions.csv',\n",
    "    'high_risk_customers.csv',\n",
    "    'high_value_opportunities.csv',\n",
    "    'action_priority_list.csv',\n",
    "    'product_recommendations.csv',\n",
    "    'product_association_rules.csv',\n",
    "    'cross_sell_opportunities.csv',\n",
    "    'rfm_segment_summary.csv',\n",
    "    'cluster_summary.csv',\n",
    "    'model_summary.csv',\n",
    "    'summary_statistics.csv',\n",
    "    'churn_model.pkl',\n",
    "    'clv_model.pkl'\n",
    "]\n",
    "\n",
    "for i, file in enumerate(generated_files, 1):\n",
    "    print(f\"  {i}. {file}\")\n",
    "\n",
    "print(\"\\n📊 VISUALIZATIONS CREATED:\")\n",
    "viz_files = [\n",
    "    'eda_dashboard.png',\n",
    "    'deep_dive_analysis.png',\n",
    "    'rfm_segmentation.png',\n",
    "    'churn_prediction_analysis.png',\n",
    "    'clv_prediction_analysis.png',\n",
    "    'customer_priority_matrix.png',\n",
    "    'recommendation_analysis.png',\n",
    "    'cross_sell_opportunities.png'\n",
    "]\n",
    "\n",
    "for i, file in enumerate(viz_files, 1):\n",
    "    print(f\"  {i}. {file}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✅ COMPLETE! ALL ANALYSIS AND FILES GENERATED SUCCESSFULLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n🎯 NEXT STEPS:\")\n",
    "print(\"  1. Review all generated CSV files for insights\")\n",
    "print(\"  2. Run the dashboard: streamlit run afrimash_dashboard.py\")\n",
    "print(\"  3. Check high_risk_customers.csv for immediate actions\")\n",
    "print(\"  4. Implement recommendations from action_priority_list.csv\")\n",
    "print(\"\\n🌾 Afrimash Customer Intelligence Solution Ready! 🚀\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
